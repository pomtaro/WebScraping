{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime as dt\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from googletrans import Translator\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "翻訳インスタンス作成\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: src_str\n",
    "return: str\n",
    "\"\"\"\n",
    "\n",
    "def remove_emoji(src_str):\n",
    "    return ''.join(c for c in src_str if c not in emoji.UNICODE_EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: src_str\n",
    "return: translated_str\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def translate_ch_to_ja(src_str):\n",
    "    src_str = remove_emoji(src_str)\n",
    "    translated_str = translator.translate(src_str, dest='ja')\n",
    "    \n",
    "    return translated_str.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: url\n",
    "return: html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_feed_html_soup(driver, url):  # 使用しない\n",
    "    retries = 3\n",
    "    i = 0\n",
    "    \n",
    "    while i < retries:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            html = driver.page_source.encode('utf-8')\n",
    "            html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            return html_soup\n",
    "        except TimeoutException:\n",
    "            i += 1\n",
    "            print('Timeout, Retrying... {} / {}'.format(i, retries))\n",
    "            driver.refresh()\n",
    "            print('driver was refreshed')\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_feed_html_soup_newdriver(driver, url):\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            driver = driver\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            \n",
    "            # sentenceの全文表示\n",
    "            # open_fullsentence(driver)\n",
    "            \n",
    "            html = driver.page_source.encode('utf-8')\n",
    "            html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "            return driver, html_soup\n",
    "    \n",
    "        except TimeoutException:\n",
    "            driver.quit()\n",
    "            driver = set_driver()\n",
    "            timeout = 30\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            driver, redirect_is = login(driver, url, 'feed')  # ログイン\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_soup\n",
    "return: mid_list\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_mid_list(html_soup):\n",
    "    mid_list = []\n",
    "    div_elements = html_soup.find_all('div', {'id': 'pl_feedlist_index'})\n",
    "    div_elements = div_elements[0].find_all('div', {'class': 'card-wrap'})\n",
    "    for elem in div_elements:\n",
    "        mid = elem.get('mid')\n",
    "        if type(mid) == str:\n",
    "            mid_list.append(mid)\n",
    "    \n",
    "    return mid_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: mid\n",
    "return: account_name, feed_time, sentence, share, comment, good\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_feed_info(html_soup, mid):\n",
    "    div_mid = html_soup.find_all('div', {'mid': mid})\n",
    "    \n",
    "    # アカウント名\n",
    "    account_name = div_mid[0].find('p').get('nick-name')\n",
    "    \n",
    "    # 時刻\n",
    "    feed_time = ''.join(div_mid[0].find_all('p', {'class': 'from'})[0].find('a').text.split())\n",
    "    try:\n",
    "        if '今天' in feed_time:     \n",
    "            feed_time = dt.strftime(dt.now(), '%Y-%m-%d ') + feed_time.split('今天')[1] + ':00'\n",
    "        elif '前' in feed_time:\n",
    "            feed_time = dt.strftime(dt.now(), '%Y-%m-%d %H:%M:%S')\n",
    "        elif '秒' in feed_time:\n",
    "            feed_time = dt.strftime(dt.now(), '%Y-%m-%d %H:%M:%S')\n",
    "        elif len(feed_time) < 12:\n",
    "            feed_time = dt.strptime(feed_time, '%m月%d日%H:%M').replace(year=2019)\n",
    "        else:\n",
    "            feed_time = dt.strptime(feed_time, '%Y年%m月%d日%H:%M')\n",
    "    except:\n",
    "        feed_time = 'not detected'\n",
    "        \n",
    "    # 本文\n",
    "    try:\n",
    "        sentence = ''.join(div_mid[0].find_all('p', {'node-type': 'feed_list_content_full'})[0].text.split('收起全文d')[0])\n",
    "    except:\n",
    "        sentence = ''.join(div_mid[0].find('p').text.split())\n",
    "    \n",
    "    # 本文、日本語訳\n",
    "    try:\n",
    "        sentence_ja = translate_ch_to_ja(sentence)\n",
    "    except:\n",
    "        sentence_ja = 'could not translate.'\n",
    "    \n",
    "    # 投稿URL\n",
    "    post_url = div_mid[0].find('p', {'class': 'from'}).find('a').get('href')\n",
    "    post_url = 'https:' + post_url\n",
    "        \n",
    "    # シェア、コメント、いいね\n",
    "    share = ''\n",
    "    comment = ''\n",
    "    suda_data = ''\n",
    "    good = ''\n",
    "    bottom_items = div_mid[0].find_all('div', {'class': 'card-act'})[0].find_all('li')\n",
    "    for bottom_item in bottom_items:\n",
    "        \n",
    "        if bottom_item.text == '收藏':\n",
    "            pass\n",
    "        elif '转发' in bottom_item.text:\n",
    "            if len(bottom_item.text.split()) < 2:\n",
    "                share = '0'\n",
    "            else:\n",
    "                share = bottom_item.text.split()[1]\n",
    "        elif '评论' in bottom_item.text:\n",
    "            if len(bottom_item.text.split()) < 2:\n",
    "                comment = '0'\n",
    "            else:\n",
    "                comment = bottom_item.text.split()[1]\n",
    "                suda_data = bottom_item.find_all('a')[0].get('suda-data')\n",
    "        else:\n",
    "            if len(bottom_item.text.split()) < 1:\n",
    "                good = '0'\n",
    "            else:\n",
    "                good = bottom_item.text   \n",
    "    \n",
    "    return account_name, feed_time, sentence, sentence_ja, share, comment, suda_data, good, post_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_soup\n",
    "return: comment_id_list\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_comment_id_list(html_soup):\n",
    "    comment_id_list = []\n",
    "    div_elements = html_soup.select('div')\n",
    "\n",
    "    for elem in div_elements:\n",
    "        comment_id = elem.get('comment_id')\n",
    "        if type(comment_id) == str:\n",
    "            if comment_id in comment_id_list:\n",
    "                continue\n",
    "            else:\n",
    "                comment_id_list.append(comment_id)\n",
    "    \n",
    "    return comment_id_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: comment_id\n",
    "return: comment_account_name, comment_text\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_comment_name_sentence(html_soup, comment_id):\n",
    "    div_comment = html_soup.find_all('div', {'comment_id': comment_id})\n",
    "    comment_account_name = div_comment[0].find('div', {'class': 'txt'}).text.split()[0]\n",
    "    comment_sentence = ''\n",
    "    if len(div_comment[0].find('div', {'class': 'txt'}).text.split()) == 3:\n",
    "        comment_sentence = div_comment[0].find('div', {'class': 'txt'}).text.split()[2]\n",
    "    elif len(div_comment[0].find('div', {'class': 'txt'}).text.split()) == 2:\n",
    "        comment_sentence = 'None or emoji'\n",
    "    return comment_account_name, comment_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_soup, mid\n",
    "return: account_link\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_account_link(driver, html_soup, mid):\n",
    "    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'name')))\n",
    "    div_mid = html_soup.find_all('div', {'mid': mid})\n",
    "    account_link = div_mid[0].find_all('a', {'class': 'name'})[0].get('href')\n",
    "    \n",
    "    return account_link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: account_link\n",
    "return: html_soup\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_account_html_soup(driver, account_link):  # 使用しない\n",
    "    retries = 3\n",
    "    i = 0\n",
    "    \n",
    "    while i < retries:\n",
    "        try:\n",
    "            driver.get(account_link)\n",
    "            time.sleep(7)  # 3sだとデータが取りきれない場合がある\n",
    "    \n",
    "            html = driver.page_source.encode('utf-8')\n",
    "            html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            return html_soup\n",
    "        \n",
    "        except TimeoutException:\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_account_html_soup_newdriver(driver, account_link):\n",
    "    i = 0\n",
    "    redirect_is = False  # weiboホームにリダイレクトされたかどうかの判定、Trueはリダイレクトあり、Falseはリダイレクトなし\n",
    "    \n",
    "    while True:\n",
    "        if i == 0:\n",
    "            pass\n",
    "        elif i == 3:\n",
    "            return driver, None\n",
    "        elif redirect_is:\n",
    "            print('redirect_is = True')\n",
    "            return driver, None\n",
    "        else:\n",
    "            driver = driver\n",
    "            \n",
    "        try:\n",
    "            driver.get(account_link)\n",
    "            \n",
    "            # 性別アイコンの表示を待つ\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'icon_bed')))\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, 'i')))\n",
    "            # フォロー、フォロワー、weiboの表示を待つ\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'S_line1')))\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.TAG_NAME, 'strong')))\n",
    "            # プロフィール情報の表示を待つ\n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'PCD_person_info')))\n",
    "            \n",
    "            html = driver.page_source.encode('utf-8')\n",
    "            html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "            return driver, html_soup\n",
    "    \n",
    "        except:\n",
    "            driver.quit()\n",
    "            driver = set_driver()\n",
    "            timeout = 30\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            driver, redirect_is = login(driver, account_link, 'account')  # ログイン\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_soup\n",
    "return: rank, location, gender, follow, follower, weibo, get_time\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_account_info(html_soup):\n",
    "    rank = get_rank(html_soup)\n",
    "    location = get_location(html_soup)\n",
    "    gender = get_gender(html_soup)\n",
    "    follow, follower, weibo = get_follow_follower_weibo(html_soup)\n",
    "    get_time = get_now_time()\n",
    "    \n",
    "    return rank, location, gender, follow, follower, weibo, get_time\n",
    "\n",
    "\n",
    "def failed_account_info():\n",
    "    rank = 'None'\n",
    "    location = 'None'\n",
    "    gender = 'None'\n",
    "    follow = 'None'\n",
    "    follower = 'None'\n",
    "    weibo = 'None'\n",
    "    get_time = 'None'\n",
    "    return rank, location, gender, follow, follower, weibo, get_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_Soup\n",
    "return: rank\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_rank(html_soup):\n",
    "    span_all = html_soup.find_all('span')\n",
    "\n",
    "    rank = ''\n",
    "\n",
    "    for span_tag in span_all:\n",
    "        if \"Lv\" in span_tag.text:\n",
    "            rank = span_tag.text\n",
    "        \n",
    "    if rank == '':\n",
    "        rank = 'no rank'\n",
    "        \n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_soup\n",
    "return: location\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_location(html_soup):\n",
    "    if html_soup.find_all('em', {'class': 'W_ficon ficon_cd_place S_ficon'}):\n",
    "        span = html_soup.find_all('span', {'class': 'item_text W_fl'})\n",
    "\n",
    "        location = ''\n",
    "\n",
    "        for tag in span:\n",
    "            if 'Lv' in tag.text:\n",
    "                location = span[1].text.split()\n",
    "                break\n",
    "            else:\n",
    "                location = span[0].text.split()\n",
    "\n",
    "        if type(location) == list:\n",
    "            word_concat = ''\n",
    "            for word in location:\n",
    "                word_concat += word\n",
    "            location = word_concat\n",
    "    else:\n",
    "        location = 'no location'\n",
    "        \n",
    "    return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_soup\n",
    "return: gender\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_gender(html_soup):\n",
    "\n",
    "    if html_soup.find_all('i', {'class': 'W_icon icon_pf_male'}):\n",
    "        gender = 'male'\n",
    "    elif html_soup.find_all('i', {'class': 'W_icon icon_pf_female'}):\n",
    "        gender = 'female'\n",
    "    else:\n",
    "        gender = 'no gender'\n",
    "        \n",
    "    return gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: html_soup\n",
    "return: follow, follower, weibo\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_follow_follower_weibo(html_soup):\n",
    "    if html_soup.find_all('strong'):\n",
    "        strongs = html_soup.find_all('strong')\n",
    "        try:\n",
    "            follow = strongs[0].text\n",
    "        except:\n",
    "            follow = 'no follow'\n",
    "        \n",
    "        try:\n",
    "            follower = strongs[1].text\n",
    "        except:\n",
    "            follower = 'no follower'\n",
    "        \n",
    "        try:\n",
    "            weibo = strongs[2].text\n",
    "        except:\n",
    "            weibo = 'no weibo'\n",
    "    else:\n",
    "        follow = 'no follow'\n",
    "        follower = 'no follower'\n",
    "        weibo = 'no weibo'\n",
    "        \n",
    "    return follow, follower, weibo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: None\n",
    "return: now_time\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_now_time():\n",
    "    now = dt.strftime(dt.now(), '%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: None\n",
    "return: driver\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def set_driver():\n",
    "    options = Options()\n",
    "\n",
    "    options.binary_location = \"/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary\"\n",
    "    options.add_argument(\"--headless\")\n",
    "\n",
    "    chromedriver_path = \"/Users/higashi/Desktop/Document/chromedriver/chromedriver\"\n",
    "\n",
    "    return webdriver.Chrome(options=options, executable_path=chromedriver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: url\n",
    "return: urls\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_feed_links_old(start_url):\n",
    "    urls = []\n",
    "    urls.append(start_url)\n",
    "    \n",
    "    url = start_url\n",
    "    \n",
    "    for i in range(200):  # 何ページ目まで読むか\n",
    "        r = requests.get(url)\n",
    "        html_contents = r.text\n",
    "\n",
    "        html_soup = BeautifulSoup(html_contents)\n",
    "        try:\n",
    "            next_link = 'https://s.weibo.com' + html_soup.find_all('a', {'class': 'next'})[0].get('href')\n",
    "            if next_link in urls:\n",
    "                continue\n",
    "            else:\n",
    "                urls.append(next_link)\n",
    "                url = next_link\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_feed_links(driver, start_url):\n",
    "    urls = []\n",
    "    urls.append(start_url)\n",
    "    \n",
    "    while True:\n",
    "        if len(urls) == 50:\n",
    "            return urls\n",
    "        try:\n",
    "            driver.get(urls[-1])\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'm-page')))\n",
    "            element = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'next')))\n",
    "            element.click()\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'm-page')))\n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'next')))\n",
    "            current_url = driver.current_url\n",
    "            urls.append(current_url)\n",
    "            \n",
    "        except:\n",
    "            current_url = driver.current_url\n",
    "            urls.append(current_url)\n",
    "            \n",
    "            return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: driver\n",
    "return: elems, elems_suda_data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_comment_button_list(driver, url):\n",
    "    elems = []\n",
    "    elems_suda_data = []\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            driver = driver\n",
    "            \n",
    "        try:\n",
    "            element = driver.find_elements_by_tag_name('a')\n",
    "            for elem in element:\n",
    "                try:\n",
    "                    val_string = elem.get_attribute('action-type')\n",
    "                    if val_string == 'feed_list_comment':\n",
    "                        elems.append(elem)\n",
    "                        elems_suda_data.append(elem.get_attribute('suda-data'))\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return driver, elems, elems_suda_data\n",
    "        \n",
    "        except:\n",
    "            driver.quit()\n",
    "            driver = set_driver()\n",
    "            timeout = 30\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            driver, redirect_is = login(driver, url, 'feed')  # ログイン\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: suda_data, elems, elems_suda_data\n",
    "return: None\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def comment_display_is(driver):\n",
    "    try:\n",
    "        html = driver.page_source.encode('utf-8')\n",
    "        html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "        feed_list_repeat = html_soup.find_all('div', {'node-type': 'feed_list_repeat'})\n",
    "        comment_display_is = feed_list_repeat[0].get('style')\n",
    "        return comment_display_is\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def click_comment_button(driver, suda_data, elems, elems_suda_data, action_type):\n",
    "    \n",
    "    if action_type == 'open':\n",
    "        click_comment(suda_data, elems, elems_suda_data)\n",
    "        while True:\n",
    "            if comment_display_is(driver) == 'display: none;':\n",
    "                time.sleep(0.2)\n",
    "                print('sleep')  # 2つめのmidでsleepループに入る\n",
    "            else:\n",
    "                break\n",
    "        driver = driver\n",
    "    elif action_type == 'close':\n",
    "        click_comment(suda_data, elems, elems_suda_data)\n",
    "        while True:\n",
    "            if comment_display_is(driver) == 'display: none;':\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(0.2)\n",
    "                print('sleep')\n",
    "        driver = driver\n",
    "    return driver\n",
    "\n",
    "    \n",
    "def click_comment(suda_data, elems, elems_suda_data):\n",
    "    try:\n",
    "        for i, elem in enumerate(elems):\n",
    "            if suda_data == elems_suda_data[i]:\n",
    "                element = elem\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        element.click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: driver, url, url_type\n",
    "return: driver\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def login(driver, url, url_type):\n",
    "    i = 0\n",
    "    feed_login_xpath = '//*[@id=\"weibo_top_public\"]/div/div/div[3]/div[2]/ul/li[3]/a'\n",
    "    account_login_xpath = '//*[@id=\"pl_common_top\"]/div/div/div[3]/div[2]/ul/li[3]/a'\n",
    "    \n",
    "    while True:\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            driver = driver\n",
    "        \n",
    "        try:  # timeout対策\n",
    "            driver.get(url)\n",
    "            \n",
    "            WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, 'S_txt1')))\n",
    "            \n",
    "            try:\n",
    "                # リダイレクトされた時のurlを確認、ここに置かないとcurrent_urlは取れない\n",
    "                current_url = driver.current_url\n",
    "                print(current_url)\n",
    "                \n",
    "                if current_url == 'https://www.weibo.com/jp':\n",
    "                    return driver, True  # リダイレクトの検知\n",
    "                else:\n",
    "                \n",
    "                    if url_type == 'feed':\n",
    "                        login_link = driver.find_element_by_xpath(feed_login_xpath)\n",
    "                        login_link.click()\n",
    "                    \n",
    "                    elif url_type == 'account':\n",
    "                        login_link = driver.find_element_by_xpath(account_login_xpath)\n",
    "                        login_link.click()\n",
    "                    \n",
    "                \n",
    "            except:\n",
    "                raise TimeoutException\n",
    "            \n",
    "            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, 'item_btn')))\n",
    "            \n",
    "            try:\n",
    "                element = driver.find_elements_by_tag_name('input')\n",
    "                for elem in element:\n",
    "                    try:\n",
    "                        val_string = elem.get_attribute('node-type')\n",
    "                        if val_string == 'username':\n",
    "                            id_input = elem\n",
    "                            id_input.send_keys('pomtaro0509@gmail.com')\n",
    "                            \n",
    "                        elif val_string == 'password':\n",
    "                            pass_input = elem\n",
    "                            pass_input.send_keys('poMtar03')\n",
    "                            \n",
    "                    except:\n",
    "                        \n",
    "                        continue\n",
    "    \n",
    "                element = driver.find_elements_by_tag_name('a')\n",
    "                for elem in element:\n",
    "                    try:\n",
    "                        val_string = elem.get_attribute('node-type')\n",
    "                        if val_string == 'submitBtn':\n",
    "                            login_button = elem\n",
    "                            login_button.click()\n",
    "                            \n",
    "                            time.sleep(10)\n",
    "                            \n",
    "                            return driver, False  # ログイン完了後のdriverを出力、リダイレクトなし\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                raise TimeoutException\n",
    "        except TimeoutException:  # timeout対策\n",
    "            driver.quit()\n",
    "            driver = set_driver()\n",
    "            timeout = 30\n",
    "            driver.set_page_load_timeout(timeout)\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s.weibo.com/weibo?q=CUREL&typeall=1&suball=1&timescope=custom:2019-03-25:2019-03-25&Refer=g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting feed links was done.\n3 links were detected.\n--progress... : 1 / 3 feed link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th mid\n2 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 th mid\n4 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 th mid\n6 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 th mid\n8 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 th mid\n10 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 th mid\n12 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 th mid\n14 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 th mid\n16 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 th mid\n18 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 th mid\n0 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/haitaozhangmenren?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/u/5109072250?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/u/5109072250?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/u/3276471320?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/curel?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-    -*-    -*-\naccount_names :  20\nlocations :  20\ngenders :  20\nranks :  20\nfollows :  20\nfollowers :  20\nweiboes :  20\nfeed_times :  20\nget_times :  20\ntype :  20\nsentences :  20\nmids :  20\nshares :  20\ncomments :  20\ngoods :  20\n-*-    -*-    -*-\nto csv was done.\n--progress... : 2 / 3 feed link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th mid\n2 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 th mid\n4 th mid\n5 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 th mid\n7 th mid\n8 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 th mid\n10 th mid\n11 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 th mid\n13 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 th mid\n15 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 th mid\n17 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 th mid\n19 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/u/3276471320?refer_flag=1001030103_&is_hot=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-    -*-    -*-\naccount_names :  40\nlocations :  40\ngenders :  40\nranks :  40\nfollows :  40\nfollowers :  40\nweiboes :  40\nfeed_times :  40\nget_times :  40\ntype :  40\nsentences :  40\nmids :  40\nshares :  40\ncomments :  40\ngoods :  40\n-*-    -*-    -*-\nto csv was done.\n--progress... : 3 / 3 feed link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th mid\n1 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 th mid\n3 th mid\n4 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 th mid\n6 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 th mid\n8 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 th mid\n10 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 th mid\n12 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 th mid\n14 th mid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 th mid\n0 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/vip1681681688?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/vip1681681688?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/u/5800559759?refer_flag=1001030103_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 th account_link\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-*-    -*-    -*-\naccount_names :  56\nlocations :  56\ngenders :  56\nranks :  56\nfollows :  56\nfollowers :  56\nweiboes :  56\nfeed_times :  56\nget_times :  56\ntype :  56\nsentences :  56\nmids :  56\nshares :  56\ncomments :  56\ngoods :  56\n-*-    -*-    -*-\nto csv was done.\ncrawling was completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "クローリング開始\n",
    "\"\"\"\n",
    "\n",
    "# フィードデータ用リスト\n",
    "account_names = []\n",
    "feed_times = []\n",
    "sentences = []\n",
    "sentences_ja = []\n",
    "shares = []\n",
    "comments = []\n",
    "goods = []\n",
    "ranks = []\n",
    "locations = []\n",
    "genders = []\n",
    "follows = []\n",
    "followers = []\n",
    "weiboes = []\n",
    "get_times = []\n",
    "types = []\n",
    "mids = []\n",
    "post_urls = []\n",
    "\n",
    "# コメントデータ用リスト\n",
    "comment_account_names = []\n",
    "comment_sentences = []\n",
    "comment_types = []\n",
    "comment_locations = []\n",
    "comment_genders = []\n",
    "comment_ranks = []\n",
    "comment_follows = []\n",
    "comment_followers = []\n",
    "comment_weiboes = []\n",
    "comment_post_times = []\n",
    "comment_get_times = []\n",
    "comment_shares = []\n",
    "comment_comments = []\n",
    "comment_goods = []\n",
    "comment_mids = []\n",
    "\n",
    "# プログラム内保持データリスト\n",
    "account_links = []\n",
    "comment_id_list = []\n",
    "\n",
    "# スタートリンクの定義\n",
    "start_url = 'https://s.weibo.com/weibo?q=CUREL&typeall=1&suball=1&timescope=custom:2019-03-25:2019-03-25&Refer=g'\n",
    "\n",
    "# driver初期化\n",
    "timeout = 30\n",
    "driver = set_driver()\n",
    "driver.set_page_load_timeout(timeout)\n",
    "\n",
    "# ログイン\n",
    "driver, redirect_is = login(driver, start_url, 'feed')\n",
    "\n",
    "# フィードのリンクを全て取得\n",
    "feed_links = get_feed_links(driver, start_url)\n",
    "print('getting feed links was done.')\n",
    "print('{} links were detected.'.format(len(feed_links)))\n",
    "\n",
    "# フィードページごとにデータ取得\n",
    "for i, feed_link in enumerate(feed_links):\n",
    "    print('--progress... : {} / {} feed link'.format((i+1), len(feed_links)))\n",
    "    \n",
    "    # フィードのhtmlを取得\n",
    "    driver, feed_html_soup = get_feed_html_soup_newdriver(driver, feed_link)\n",
    "    \n",
    "    if feed_html_soup:\n",
    "        # midリストを取得\n",
    "        mid_list = get_mid_list(feed_html_soup)\n",
    "        \n",
    "        # commentボタンリストを取得\n",
    "        driver, elems, elems_suda_data = get_comment_button_list(driver, feed_link)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # 初期化\n",
    "    account_links.clear()\n",
    "        \n",
    "    # commentデータを先に取得\n",
    "    for j, mid in enumerate(mid_list):\n",
    "        print('{} th mid'.format(j))\n",
    "        # アカウントリンクを取得\n",
    "        account_link = 'https:' + get_account_link(driver, feed_html_soup, mid)\n",
    "        account_links.append(account_link)\n",
    "               \n",
    "        # フィードデータを取得\n",
    "        account_name, feed_time, sentence, sentence_ja, share, comment, suda_data, good, post_url = get_feed_info(feed_html_soup, mid)\n",
    "        \n",
    "        \"\"\"コメントを取得する場合は以下のコメントアウトを解除\n",
    "        # コメントが存在する場合、コメントを取得しに行く\n",
    "        if comment != '0' and elems is not None and elems_suda_data is not None:\n",
    "            \n",
    "            click_comment(suda_data, elems, elems_suda_data)\n",
    "            print('opened')\n",
    "        \n",
    "            html = driver.page_source.encode('utf-8')\n",
    "            html_soup = BeautifulSoup(html, \"html.parser\")\n",
    "            comment_id_list = get_comment_id_list(html_soup)\n",
    "            print('-*-*-')\n",
    "            print(comment)\n",
    "            print(comment_id_list)\n",
    "            print('-*-*-')\n",
    "            \n",
    "            click_comment(suda_data, elems, elems_suda_data)  # コメント欄を閉じておく\n",
    "            print('closed')\n",
    "            # 宣言\n",
    "            global comment_account_names, comment_sentences, comment_types, comment_locations\n",
    "            global comment_genders, comment_ranks, comment_follows, comment_followers\n",
    "            global comment_weiboes, comment_post_times, comment_get_times, comment_shares\n",
    "            global comment_comments, comment_goods, comment_mids\n",
    "            mid = mid\n",
    "            \n",
    "            # コメントデータを取得、格納\n",
    "            for comment_id in comment_id_list:  \n",
    "                # データ取得\n",
    "                comment_account_name, comment_sentence = get_comment_name_sentence(html_soup, comment_id)\n",
    "                \n",
    "                # データ格納\n",
    "                comment_account_names.append(comment_account_name)\n",
    "                comment_sentences.append(comment_sentence)\n",
    "                comment_types.append('comment')\n",
    "                comment_locations.append('None')\n",
    "                comment_genders.append('None')\n",
    "                comment_ranks.append('None')\n",
    "                comment_follows.append('None')\n",
    "                comment_followers.append('None')\n",
    "                comment_weiboes.append('None')\n",
    "                comment_post_times.append('None')\n",
    "                comment_get_times.append('None')\n",
    "                comment_shares.append('None')\n",
    "                comment_comments.append('None')\n",
    "                comment_goods.append('None')\n",
    "                comment_mids.append(mid)\n",
    "        \"\"\"\n",
    "        \n",
    "        # フィードデータの一部を格納\n",
    "        account_names.append(account_name)\n",
    "        feed_times.append(feed_time)\n",
    "        sentences.append(sentence)\n",
    "        sentences_ja.append(sentence_ja)\n",
    "        shares.append(share)\n",
    "        comments.append(comment)\n",
    "        goods.append(good)\n",
    "        mids.append(mid)\n",
    "        post_urls.append(post_url)\n",
    "        \n",
    "        # print('----storing feed data was done.')\n",
    "    \n",
    "    # アカウント情報を取得\n",
    "    for k, account_link in enumerate(account_links):\n",
    "        \n",
    "        print('{} th account_link'.format(k))\n",
    "        \n",
    "        # アカウント詳細のhtmlを取得\n",
    "        driver, account_html_soup = get_account_html_soup_newdriver(driver, account_link)\n",
    "        # print('----getting account html_soup was done.')\n",
    "        \n",
    "        # アカウントデータを取得\n",
    "        if account_html_soup:\n",
    "            rank, location, gender, follow, follower, weibo, get_time = get_account_info(account_html_soup)\n",
    "            # print('----getting account info was done.')\n",
    "        else:\n",
    "            rank, location, gender, follow, follower, weibo, get_time = failed_account_info()\n",
    "            \n",
    "        # タイプを定義\n",
    "        type_name = 'feed'\n",
    "        \n",
    "        # フィードデータ格納\n",
    "        ranks.append(rank)\n",
    "        locations.append(location)\n",
    "        genders.append(gender)\n",
    "        follows.append(follow)\n",
    "        followers.append(follower)\n",
    "        weiboes.append(weibo)\n",
    "        get_times.append(get_time)\n",
    "        types.append(type_name)\n",
    "        \n",
    "    os.chdir('/Users/higashi/PycharmProjects/WebScraping/venv/weibo/data')  # csv保存\n",
    "\n",
    "    print('-*-    -*-    -*-')\n",
    "    print('account_names : ', len(account_names))\n",
    "    print('locations : ', len(locations))\n",
    "    print('genders : ', len(genders))\n",
    "    print('ranks : ', len(ranks))\n",
    "    print('follows : ', len(follows))\n",
    "    print('followers : ', len(followers))\n",
    "    print('weiboes : ', len(weiboes))\n",
    "    print('feed_times : ', len(feed_times))\n",
    "    print('get_times : ', len(get_times))\n",
    "    print('type : ', len(types))\n",
    "    print('sentences : ', len(sentences))\n",
    "    print('mids : ', len(mids))\n",
    "    print('shares : ', len(shares))\n",
    "    print('comments : ', len(comments))\n",
    "    print('goods : ', len(goods))\n",
    "    print('-*-    -*-    -*-')\n",
    "\t\n",
    "    df_feed = pd.DataFrame({\n",
    "\t\t'Account': account_names,\n",
    "\t\t'Follower': followers,\n",
    "\t\t'Share': shares,\n",
    "\t\t'FeedTime': feed_times,\n",
    "\t\t'Comment': comments,\n",
    "\t\t'Good': goods,\n",
    "\t\t'Sentence': sentences,\n",
    "\t\t'PostUrl': post_urls,\n",
    "\t\t'Sentence_Ja': sentences_ja,\n",
    "\t\t'Location': locations,\n",
    "\t\t'Gender': genders,\n",
    "\t\t'Rank': ranks,\n",
    "\t\t'Follow': follows,\n",
    "\t\t'Weibo': weiboes,\n",
    "\t\t'GetTime': get_times,\n",
    "\t\t'Type': types,\n",
    "\t\t'Mid': mids,\n",
    "\t})\n",
    "\t\n",
    "    df_comment = pd.DataFrame({\n",
    "\t\t'Account': comment_account_names,\n",
    "\t\t'Location': comment_locations,\n",
    "\t\t'Gender': comment_genders,\n",
    "\t\t'Rank': comment_ranks,\n",
    "\t\t'Follow': comment_follows,\n",
    "\t\t'Follower': comment_followers,\n",
    "\t\t'Weibo': comment_weiboes,\n",
    "\t\t'FeedTime': comment_post_times,\n",
    "\t\t'GetTime': comment_get_times,\n",
    "\t\t'Type': comment_types,\n",
    "\t\t'Sentence': comment_sentences,\n",
    "\t\t'ParentMid': comment_mids,\n",
    "\t\t'Share': comment_shares,\n",
    "\t\t'Comment': comment_comments,\n",
    "\t\t'Good': comment_goods\n",
    "\t})\n",
    "\t\n",
    "    # df_feed.to_csv('20190319_weibo_freeplus_20190116-20190228_feed' + '.csv')\n",
    "    # df_comment.to_csv('20190319_weibo_freeplus__20190116-20190228_comment' + '.csv')\n",
    "\t\n",
    "    df_feed.to_csv('tmp_feed' + '.csv')\n",
    "    df_comment.to_csv('tmp_comment' + '.csv')\n",
    "    print('to csv was done.')\n",
    "\t\n",
    "\t\n",
    "print('crawling was completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4336742938665912', '4321546749152705', '4321546559972033', '4321381229575291', '4321381208054759', '4321264225062097', '4321248362324656', '4321214073587426', '4321209712233985', '4321163804787424', '4314560285012915', '4314366373073469', '4313094187118583', '4314675052843889', '4312159074660122', '4311027174640643', '4311019675411550', '4311013887433861', '4310986321899985', '4310978164256330', '4307359276894482', '4306188935892850']\n"
     ]
    }
   ],
   "source": [
    "print(comment_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "データ数の確認\n",
    "\"\"\"\n",
    "\n",
    "print('account_names : ', len(account_names))\n",
    "print('feed_times : ', len(feed_times))\n",
    "print('sentences : ', len(sentences))\n",
    "print('shares : ', len(shares))\n",
    "print('comments : ', len(comments))\n",
    "print('goods : ', len(goods))\n",
    "print('mids', len(mids))\n",
    "\n",
    "print('-*-    -*-    -*-')\n",
    "\n",
    "print('locations : ', len(locations))\n",
    "print('genders : ', len(genders))\n",
    "print('ranks : ', len(ranks))\n",
    "print('follows : ', len(follows))\n",
    "print('followers : ', len(followers))\n",
    "print('weiboes : ', len(weiboes))\n",
    "print('get_times : ', len(get_times))\n",
    "\n",
    "print('-*-    -*-    -*-')\n",
    "\n",
    "print('comment_shares : ', len(comment_shares))\n",
    "print('comment_comments : ', len(comment_comments))\n",
    "print('comment_goods', len(comment_goods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/higashi/PycharmProjects/WebScraping/venv/weibo/data')\n",
    "\n",
    "df_feed = pd.DataFrame({\n",
    "    'Account': account_names,\n",
    "    'Location': locations,\n",
    "    'Gender': genders,\n",
    "    'Rank': ranks,\n",
    "    'Follow': follows,\n",
    "    'Follower': followers,\n",
    "    'Weibo': weiboes,\n",
    "    'FeedTime': feed_times,\n",
    "    'GetTime': get_times,\n",
    "    'Type': types,\n",
    "    'Sentence': sentences,\n",
    "    'Mid': mids,\n",
    "    'Share': shares,\n",
    "    'Comment': comments,\n",
    "    'Good': goods\n",
    "})\n",
    "\n",
    "df_comment = pd.DataFrame({\n",
    "    'Account': comment_account_names,\n",
    "    'Location': comment_locations,\n",
    "    'Gender': comment_genders,\n",
    "    'Rank': comment_ranks,\n",
    "    'Follow': comment_follows,\n",
    "    'Follower': comment_followers,\n",
    "    'Weibo': comment_weiboes,\n",
    "    'FeedTime': comment_post_times,\n",
    "    'GetTime': comment_get_times,\n",
    "    'Type': comment_types,\n",
    "    'Sentence': comment_sentences,\n",
    "    'ParentMid': comment_mids,\n",
    "    'Share': comment_shares,\n",
    "    'Comment': comment_comments,\n",
    "    'Good': comment_goods\n",
    "})\n",
    "\n",
    "df_feed.to_csv('20190319_weibo_CUREL_20190101-20190115_feed' + '.csv')\n",
    "df_comment.to_csv('20190319_weibo_CUREL__20190101-20190115_comment' + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(account_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(account_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://passport.weibo.com/visitor/visitor?entry=miniblog&a=enter&url=https%3A%2F%2Fweibo.com%2F87768787%3Frefer_flag%3D1001030103_&domain=.weibo.com&ua=php-sso_sdk_client-0.6.28&_rand=1552888160.1266'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(account_links[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = set_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_url = 'https://www.weibo.com/shengmingshibao?refer_flag=0000015010_&from=feed&loc=nickname'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for account_url in account_links:\n",
    "    driver.get(account_url)\n",
    "    driver, html_soup = get_account_html_soup_newdriver(driver, account_url)\n",
    "    print(get_account_info(html_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(account_links[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver, html_soup = get_account_html_soup_newdriver(driver, account_links[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_account_info(html_soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://s.weibo.com/weibo/d%2520program?topnav=1&wvr=6&b=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver, html_soup = get_feed_html_soup_newdriver(driver, 'https://s.weibo.com/weibo/d%2520program?topnav=1&wvr=6&b=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(feed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feed_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.append('dummy')\n",
    "genders.append('dummy')\n",
    "ranks.append('dummy')\n",
    "follows.append('dummy')\n",
    "followers.append('dummy')\n",
    "weiboes.append('dummy')\n",
    "get_times.append('dummy')\n",
    "types.append('dummy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['小L专属空瓶记', '大妞日代-微信43891819', '竹内柚子膏', 'RineVa', 'Raku桑', '小懒分析', 'BoJapan播日本', '叮当爱红茶奶酥不爱铜锣烧', '我是小叶日本代购', '二陈卵子', '功课菌', '锅崽崽_JP', '刘二二人肉正品专柜日本韩国代购', '日本流行每日速报', '我是小叶日本代购', '庞老板是大胖子日代小铺', '起航转运', '要健康开心呀', '未来的温妍', '日本最潮FM-Channel', '网上小玖', '网上小玖', '大可离不开紫薯君', '今天阿曼达翻白眼了吗', '墩布挖白菜', '我是山大jane', '肥鼠日本代_购', '卍岁娘', 'LILI莉丽日本代购', '我看谁还能找到我', '乾乾有只猫', '搞去念三_', '绿野仙踪Philly', 'lllaan_', '是wasabi啊_', '卍岁娘', '金韩彬脸上的创可贴', 'Echo__sssss', '小L专属空瓶记', '019兔斯基是我本命的beauty', '日本代购各类品牌护肤品', '日本代购各类品牌护肤品', 'pummmpk1n', '苗苗苗苗苗儿呀', '于小小小菲1224', '芽绿酱', '躲進夢裡', '是蛋挞君的日代小铺阿', '小猫咪呀小猫咪你真可爱呀', 'zzZmm是不合格的铲屎官', 'itsKauteki']\n"
     ]
    }
   ],
   "source": [
    "print(account_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = set_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.weibo.com/jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/jp\n"
     ]
    }
   ],
   "source": [
    "driver = set_driver()\n",
    "driver = login(driver, 'https://www.weibo.com/jp', 'account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weibo.com/jp\nredirect_is = True\n"
     ]
    }
   ],
   "source": [
    "driver, html_soup = get_account_html_soup_newdriver(driver, 'https://www.weibo.com/jp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = 'https://s.weibo.com/weibo/CUREL?topnav=1&wvr=6&b=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = set_driver()\n",
    "driver.get(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = driver.find_elements_by_tag_name('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicked\n"
     ]
    }
   ],
   "source": [
    "for element in elements:\n",
    "    val_string = element.get_attribute('action-type')\n",
    "    if val_string == 'fl_unfold':\n",
    "        element.click()\n",
    "        print('clicked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arg: driver\n",
    "return: \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def open_fullsentence(driver):\n",
    "    elements = driver.find_elements_by_tag_name('a')\n",
    "    \n",
    "    for element in elements:\n",
    "        val_string = element.get_attribute('action-type')\n",
    "        if val_string == 'fl_unfold':\n",
    "            element.click()\n",
    "            print('opened sentence')\n",
    "            \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = '黒柳徹子渡辺直美'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = '直美'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['黒柳徹子渡辺', '']\n"
     ]
    }
   ],
   "source": [
    "print(A.split(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated(src=zh-CN, dest=ja, text=＃日代推薦##頼符日代＃ \"スポット\" Curelカット啫ジェルリムーバーハニー130g、70。敏感肌の特別な妊娠中の母親もアンロード後に乳化効果が非常に良いです使用することができ、肌は完全にオイルフリーの押出は透明、白化やさわやかではない、特に肌にアレルギー性乾燥肌に適しています。, pronunciation=None, extra_data=\"{'translat...\")\n"
     ]
    }
   ],
   "source": [
    "print(translator.translate(remove_emoji(emojis), dest='ja'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(src_str):\n",
    "    return ''.join(c for c in src_str if c not in emoji.UNICODE_EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = '#日代推荐##靠谱日代#「现货」Curel珂润啫喱卸妆蜜130g，💰70。敏感性肌肤专用孕妈妈也可以使用的乳化效果很赞✨卸完后肌肤完全不油挤出来是透明状不泛白清爽特别适合油性易起痘皮肤过敏偏干性皮肤。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#日代推荐##靠谱日代#「现货」Curel珂润啫喱卸妆蜜130g，70。敏感性肌肤专用孕妈妈也可以使用的乳化效果很赞卸完后肌肤完全不油挤出来是透明状不泛白清爽特别适合油性易起痘皮肤过敏偏干性皮肤。\n"
     ]
    }
   ],
   "source": [
    "print(remove_emoji(emojis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = translate_ch_to_ja('#日代推荐##靠谱日代#「现货」Curel珂润啫喱卸妆蜜130g，💰70。敏感性肌肤专用孕妈妈也可以使用的乳化效果很赞✨卸完后肌肤完全不油挤出来是透明状不泛白清爽特别适合油性易起痘皮肤过敏偏干性皮肤。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "＃日代推薦##頼符日代＃ \"スポット\" Curelカット啫ジェルリムーバーハニー130g、70。敏感肌の特別な妊娠中の母親もアンロード後に乳化効果が非常に良いです使用することができ、肌は完全にオイルフリーの押出は透明、白化やさわやかではない、特に肌にアレルギー性乾燥肌に適しています。\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
